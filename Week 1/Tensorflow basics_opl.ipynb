{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6348aea",
   "metadata": {},
   "source": [
    "# Tensorflow basics\n",
    "\n",
    "In deze notebook worden een aantal basisconcepten van tensorflow toegelicht en bekeken aan de hand van een aantal voorbeelden.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Het basisconcept van tensorflow is een tensor.\n",
    "Dit is een multi-dimensionele rij met een uniform datatype of dtype.\n",
    "Dit concept komt heel sterk overeen met een array van Numpy.\n",
    "Alle tensors zijn onwijzigbaar of immutable, net zoals spark dataframes.\n",
    "Dit houdt in dat hun waarden niet kunnen gewijzigd worden.\n",
    "Alle operaties op een tensor maken dus een nieuwe tensor aan.\n",
    "\n",
    "Een tensor aanmaken kan door middel van de constant functie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f45899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e4679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = tf.constant(4)\n",
    "print(tensor1)\n",
    "\n",
    "tensor2 = tf.constant([2.0, 3.0, 4.0])\n",
    "print(tensor2)\n",
    "\n",
    "tensor3 = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32)\n",
    "print(tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0f17f",
   "metadata": {},
   "source": [
    "Op tensors kunnen de volgende operaties uitgevoerd worden\n",
    "* add\n",
    "* multiply\n",
    "* matrix multiplication\n",
    "* en andere wiskunde operaties zoals softmax, maximum vinden, ... (zie hiervoor ook de [math package](https://www.tensorflow.org/api_docs/python/tf/math))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f7435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor([5. 6. 7.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 3.  6.  9.]\n",
      " [12. 15. 18.]\n",
      " [21. 24. 27.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 3.  6.  9.]\n",
      " [12. 15. 18.]\n",
      " [21. 24. 27.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 30.  36.  42.]\n",
      " [ 66.  81.  96.]\n",
      " [102. 126. 150.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 30.  36.  42.]\n",
      " [ 66.  81.  96.]\n",
      " [102. 126. 150.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor([2 2 2], shape=(3,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0.09003057 0.24472848 0.6652409 ]\n",
      " [0.09003057 0.24472848 0.6652409 ]\n",
      " [0.09003057 0.24472848 0.6652409 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#add\n",
    "print(tf.add(tensor1, 3))\n",
    "print(tf.add(tensor2, 3))\n",
    "\n",
    "#multiply\n",
    "print(tf.multiply(3, tensor3))\n",
    "print(3 * tensor3)\n",
    "\n",
    "#matrix multiplication\n",
    "print(tf.matmul(tensor3, tensor3))\n",
    "print(tensor3 @ tensor3)\n",
    "\n",
    "#find maximum\n",
    "print(tf.reduce_max(tensor3))\n",
    "\n",
    "#find index of maximum\n",
    "print(tf.math.argmax(tensor3))\n",
    "\n",
    "#compute softmax for each element in tensor\n",
    "print(tf.nn.softmax(tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2d214",
   "metadata": {},
   "source": [
    "**Shapes**\n",
    "\n",
    "De dimensies van een tensor zijn zeer belangrijk.\n",
    "Bij communicatie over de tensors worden de volgende termen gebruikt:\n",
    "* Axis: Dit is een specifieke dimensie van de tensor\n",
    "* Rank: Het aantal dimensies waarover de tensor beschikt\n",
    "    * Eenvoudig getal: 0\n",
    "    * Een enkelvoudige rij: 1\n",
    "    * Een matrix: 2\n",
    "* Shape: De lengte van elk van de dimensies van de tensor. **Let op**: Buiten een aantal uitzondering moet een tensor steeds rechthoekig zijn. Dit wil zeggen dat elk element op een axis dezelfde lengte moet hebben.\n",
    "* Size: Het totaal aantal elementen in de tensor. Dit is het product van de lengtes van elke dimensie\n",
    "\n",
    "Deze waarden van een tensor kunnen als volgt berekend worden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee746a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of every element: <dtype: 'float32'>\n",
      "Number of axes: 2\n",
      "Shape of tensor: (3, 3)\n",
      "Elements along axis 0 of tensor: 3\n",
      "Elements along the last axis of tensor: 3\n",
      "Total number of elements (3*3):  tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of every element:\", tensor3.dtype)\n",
    "print(\"Number of axes:\", tensor3.ndim)\n",
    "print(\"Shape of tensor:\", tensor3.shape)\n",
    "print(\"Elements along axis 0 of tensor:\", tensor3.shape[0])\n",
    "print(\"Elements along the last axis of tensor:\", tensor3.shape[-1])\n",
    "print(\"Total number of elements (3*3): \", tf.size(tensor3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204c059",
   "metadata": {},
   "source": [
    "**Indexing:**\n",
    "\n",
    "Tensorflow maakt gebruik van de standaard python indexing regels:\n",
    "* Indexes beginnen op 0\n",
    "* Negatieve indexes tellen vanaf het einde\n",
    "* Dubbelpunt wordt gebruikt om slices of bereiken te kiezen volgens start:stop:end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c508c819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  2  3  5  8 13 21 34]\n",
      "First: 0\n",
      "Second: 1\n",
      "Last: 34\n",
      "Everything: [ 0  1  1  2  3  5  8 13 21 34]\n",
      "Before 4: [0 1 1 2]\n",
      "From 4 to the end: [ 3  5  8 13 21 34]\n",
      "From 2, before 7: [1 2 3 5 8]\n",
      "Every other item: [ 0  1  3  8 21]\n",
      "Reversed: [34 21 13  8  5  3  2  1  1  0]\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "5.0\n",
      "Second row: [4. 5. 6.]\n",
      "Second column: [2. 5. 8.]\n",
      "Last row: [7. 8. 9.]\n",
      "First item in last column: 3.0\n",
      "Skip the first row:\n",
      "[[4. 5. 6.]\n",
      " [7. 8. 9.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
    "print(t.numpy())\n",
    "# 1 axis geselecteerd dus rank is 1 kleiner\n",
    "print(\"First:\", t[0].numpy())\n",
    "print(\"Second:\", t[1].numpy())\n",
    "print(\"Last:\", t[-1].numpy())\n",
    "# gebruik maken van de slices zorgt ervoor dat rank gelijk blijft\n",
    "print(\"Everything:\", t[:].numpy())\n",
    "print(\"Before 4:\", t[:4].numpy())\n",
    "print(\"From 4 to the end:\", t[4:].numpy())\n",
    "print(\"From 2, before 7:\", t[2:7].numpy())\n",
    "print(\"Every other item:\", t[::2].numpy())\n",
    "print(\"Reversed:\", t[::-1].numpy())\n",
    "\n",
    "# multi-axis indexing\n",
    "print()\n",
    "print(tensor3.numpy())\n",
    "print(tensor3[1, 1].numpy())\n",
    "print(\"Second row:\", tensor3[1, :].numpy())\n",
    "print(\"Second column:\", tensor3[:, 1].numpy())\n",
    "print(\"Last row:\", tensor3[-1, :].numpy())\n",
    "print(\"First item in last column:\", tensor3[0, -1].numpy())\n",
    "print(\"Skip the first row:\")\n",
    "print(tensor3[1:, :].numpy(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864f7bf",
   "metadata": {},
   "source": [
    "**Manipuleren van een tensor**\n",
    "\n",
    "Reshaping of het aanpassen van de shape van een tensor kan vaak heel handig zijn.\n",
    "Hiervoor kan je de reshape functie gebruiken. \n",
    "Deze operatie is heel rekenefficient omdat de data ongewijzigd blijft.\n",
    "\n",
    "Voor ML moet er vaak een matrix van features (figuren) omgezet worden naar een 1-dimensionale vector/rij/tensor om het te kunnen aanbieden aan een neuraal netwerk. Deze operatie wordt ook flatten genoemd.\n",
    "\n",
    "Typisch is het enkel aangeraden om een reshape te doen om dimensies met een lengte van 1 weg te laten of naburige axis te combineren of te splitsen. Indien je meer willekeurige zaken doet gaan de resultaten niet altijd bruikbaar zijn omdat de volgorde van dimensies niet gerespecteerd blijft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7094fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "(3, 1)\n",
      "\n",
      "[[1 2 3]]\n",
      "(1, 3)\n",
      "[1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1], [2], [3]])\n",
    "print(x.numpy())\n",
    "print(x.shape)\n",
    "print()\n",
    "\n",
    "x = tf.reshape(x, [1,3])\n",
    "print(x.numpy())\n",
    "print(x.shape)\n",
    "\n",
    "#flatten\n",
    "# A `-1` passed in the `shape` argument says \"Whatever fits\".\n",
    "print(tf.reshape(tensor3, [-1]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f6b60",
   "metadata": {},
   "source": [
    "Daarnaast zijn er nog een aantal interessante kenmerken van tensors die zelf verder bestudeerd kunnen worden.\n",
    "Enkele voorbeelden hiervan zijn:\n",
    "* Broadcasting\n",
    "* Data types\n",
    "* Sparse tensors\n",
    "* Ragged tensors\n",
    "* String tensors\n",
    "\n",
    "**Oefening:**\n",
    "\n",
    "Maak twee tensors aan met respectievelijk een shape van 3x4 en 4x3.\n",
    "* Bereken het product van beide matrices. Kan dit product in beide richtingen berekend worden?\n",
    "* Wat is het gemiddelde van de eerste tensor?\n",
    "* Wat is het minimum van de tweede tensor?\n",
    "* Bereken het negatieve van de eerste tensor. Dit betekend dat elk element vermenigvuldigd wordt met -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfacd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "tf.Tensor(\n",
      "[[ 42  48  54]\n",
      " [114 136 158]\n",
      " [186 224 262]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 20  23  26  29]\n",
      " [ 56  68  80  92]\n",
      " [ 92 113 134 155]\n",
      " [128 158 188 218]], shape=(4, 4), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  0  -1  -2  -3]\n",
      " [ -4  -5  -6  -7]\n",
      " [ -8  -9 -10 -11]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[  0  -1  -2  -3]\n",
      " [ -4  -5  -6  -7]\n",
      " [ -8  -9 -10 -11]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(range(12))\n",
    "tensor1 = tf.reshape(x, [3,4])\n",
    "tensor2 = tf.reshape(x, [4,3])\n",
    "print(tensor1.numpy())\n",
    "print(tensor2.numpy())\n",
    "\n",
    "print(tensor1 @ tensor2)\n",
    "print(tensor2 @ tensor1)\n",
    "\n",
    "print(tf.math.reduce_mean(tensor1))\n",
    "print(tf.math.reduce_min(tensor2))\n",
    "print(tf.math.negative(tensor1))\n",
    "print(tensor1 * -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1700537",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Met tensors kan je reeds heel wat berekeningen uitvoeren.\n",
    "Echter is het onmogleijk om een persistente state te hebben die gemanipuleerd wordt door het programma omdat ze immutable zijn.\n",
    "Deze persistente state kan bijvoorbeeld een tensor van de gewichten zijn in het model of andere parameters die gewijzigd of getrained moeten worden.\n",
    "\n",
    "Om deze state bij te houden moet er gebruikt gemaakt worden van een Variable.\n",
    "Deze kan aangemaakt worden door gebruik te maken van de klasse tf.Variable().\n",
    "Een object van deze klasse stelt een tensor voor dat wel kan aangepast worden door er operaties op uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb33e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (2, 2)\n",
      "DType:  <dtype: 'float32'>\n",
      "As NumPy:  [[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "my_variable = tf.Variable(my_tensor)\n",
    "\n",
    "# Variables can be all kinds of types, just like tensors\n",
    "bool_variable = tf.Variable([False, False, False, True])\n",
    "complex_variable = tf.Variable([5 + 4j, 6 + 1j])\n",
    "\n",
    "print(\"Shape: \", my_variable.shape)\n",
    "print(\"DType: \", my_variable.dtype)\n",
    "print(\"As NumPy: \", my_variable.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2f609",
   "metadata": {},
   "source": [
    "Zoals hierboven aangehaald moeten er specifieke operaties gebruikt worden om de waarde van de tensor aan te passen.\n",
    "Een aantal mogelijke operaties hiervoor zijn:\n",
    "* tf.Variable.assign()\n",
    "* tf.Variable.assign_add()\n",
    "* tf.Variable.assign_sub()\n",
    "* Andere functies kan je [hier](https://www.tensorflow.org/api_docs/python/tf/Variable) vinden.\n",
    "\n",
    "Let op dat hierbij de aangemaakt tensor herbruikt wordt. Hierdoor is het dus niet mogelijk om een andere size in te stellen voor de variable.\n",
    "\n",
    "In de code hieronder zie je een aantal code voorbeelden voor te werken met variabelen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34175c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible.\n",
      "[5. 6.]\n",
      "[2. 3.]\n",
      "[7. 9.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([2.0, 3.0])\n",
    "# This will keep the same dtype, float32\n",
    "a.assign([1, 2]) \n",
    "# Not allowed as it resizes the variable: \n",
    "try:\n",
    "    a.assign([1.0, 2.0, 3.0])\n",
    "except Exception as e:\n",
    "    print(f\"{type(e).__name__}: {e}\")\n",
    "    \n",
    "a = tf.Variable([2.0, 3.0])\n",
    "# Create b based on the value of a\n",
    "b = tf.Variable(a)\n",
    "a.assign([5, 6])\n",
    "\n",
    "# a and b are different\n",
    "print(a.numpy())\n",
    "print(b.numpy())\n",
    "\n",
    "# There are other versions of assign\n",
    "print(a.assign_add([2,3]).numpy())  # [7. 9.]\n",
    "print(a.assign_sub([7,9]).numpy())  # [0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8f045",
   "metadata": {},
   "source": [
    "Voor performantie probeert Tensorflow steeds de tensors en variabelen op het snelste toestel te plaatsen dat compatibel is met zijn datatype. \n",
    "Dit betekend dat de meeste variabelen op de **GPU** bewaard worden indien er 1 beschikbaar is.\n",
    "Dit gedrag kan aangepast worden doro middel van de tf.device functie.\n",
    "Indien je hiervan gebruik maakt kan je er ook voor zorgen dat de locatie ingesteld wordt op 1 toestel en de uitvoering op een ander gedaan wordt.\n",
    "Dit introduceert wel een delay omdat de data gekopieerd moet worden naar het andere toestel.\n",
    "Dit wordt soms gedaan als je meerdere GPU's gebruikt maar slechts 1 kopie wilt van de variabelen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa4a20ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [ 4. 10. 18.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "    a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.Variable([[1.0, 2.0, 3.0]])\n",
    "\n",
    "with tf.device('GPU:0'):\n",
    "    # Element-wise multiply\n",
    "    k = a * b\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95204af",
   "metadata": {},
   "source": [
    "## Eager vs graph execution\n",
    "\n",
    "In de code hierboven is steeds gebruik gemaakt van zogenoemde **eager execution**. \n",
    "Dit houdt in dat de code uitgevoerd wordt op dezelfde manier dat de python code uitgevoerd wordt, namelijk:\n",
    "* Lijn per lijn wordt de code geinterpreerd\n",
    "* Operatie na operatie wordt uitgevoerd\n",
    "* De resultaten worden na elke operatie teruggegeven aan de python interpreter\n",
    "\n",
    "Ook al heeft deze eager execution een aantal voorbeelden (met name dat het vlotter programmeert in python) zijn er ook een aantal problemen.\n",
    "Het grootste probleem is dat je code enkel kan uitgevoerd worden op een systeem dat python kan draaien.\n",
    "Daarnaast is het ook niet het meest performante systeem omdat de interpreter van python geen code kan optimaliseren doordat het lijn per lijn werkt.\n",
    "Een alternatieve manier van uitvoering dat beschikbaar gesteld wordt door tensorflow wordt **graph execution** genoemd.\n",
    "Dit houdt in dat je in python code een graaf opsteld van de operaties die voor een bepaalde input moeten uitgevoerd worden.\n",
    "Tensorflow kan dan alle operaties in de volledige graaf in 1 keer en in parallel uitvoeren.\n",
    "Meer informatie over deze manier van code uitvoeren kan je [hier](https://www.tensorflow.org/guide/intro_to_graphs) vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe899041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n",
      "[[12.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Python function.\n",
    "def a_regular_function(x, y, b):\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "    return x\n",
    "\n",
    "# maak van de python functie een graph functie by wrapping it\n",
    "a_function_that_uses_a_graph = tf.function(a_regular_function)\n",
    "\n",
    "# test de twee manieren\n",
    "x1 = tf.constant([[1.0, 2.0]])\n",
    "y1 = tf.constant([[2.0], [3.0]])\n",
    "b1 = tf.constant(4.0)\n",
    "\n",
    "orig_value = a_regular_function(x1, y1, b1).numpy()\n",
    "print(orig_value)\n",
    "tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()\n",
    "print(tf_function_value)\n",
    "assert(orig_value == tf_function_value)\n",
    "\n",
    "# tweede manier om een graph functie aan te maken\n",
    "# door middel van decorator (annotatie) @\n",
    "@tf.function\n",
    "def outer_function(x):\n",
    "    y = tf.constant([[2.0], [3.0]])\n",
    "    b = tf.constant(4.0)\n",
    "\n",
    "    return a_regular_function(x, y, b)\n",
    "\n",
    "outer_function(tf.constant([[1.0, 2.0]])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaca830",
   "metadata": {},
   "source": [
    "Een eerste belangrijke opmerking is dat een graph gemaakt wordt voor elke unieke set van inputs (dus op basis de specifieke waarden).\n",
    "Dit wordt ook wel de handtekening of (input) signature genoemd van de graph.\n",
    "Dit is belangrijk omdat op deze manier de resultaten gecached kunnen worden en de optimalisatie niet steeds opnieuw moet gebeuren.\n",
    "\n",
    "De omzetting van eager execution naar graph execution gaat vrij automatisch en probleemloos.\n",
    "De meeste standaard python code wordt zonder problemen en volledig omgezet.\n",
    "Een eerste belangrijke opmerking is hoe de graph execution werkt.\n",
    "Bekijk hiervoor de output van onderstaande voorbeeld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f8b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 7 9 3 9], shape=(5,), dtype=int32)\n",
      "tf.Tensor([4 7 4 2 4], shape=(5,), dtype=int32)\n",
      "Calculating Mean Square Error!\n",
      "Calculating Mean Square Error!\n",
      "Calculating Mean Square Error!\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def get_MSE(y_true, y_pred):\n",
    "    print(\"Calculating Mean Square Error!\")\n",
    "    sq_diff = tf.pow(y_true - y_pred, 2)\n",
    "    return tf.reduce_mean(sq_diff)\n",
    "\n",
    "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "# force eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "error = get_MSE(y_true, y_pred)\n",
    "\n",
    "# tf.config.run_functions_eagerly(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20504034",
   "metadata": {},
   "source": [
    "Het valt op dat als je bovenstaande code uitvoert als graph execution, dat de output slechts 1 keer uitgeprint wordt. \n",
    "Dit komt omdat de graph geoptimaliseerd wordt door het \"tracing\" proces.\n",
    "De print wordt hierdoor niet meegenomen omdat het geen deel uitmaakt van de tensorflow operaties en ook geen impact heeft.\n",
    "De print functie wordt ook wel een python side effect genoemd.\n",
    "Als je toch steeds een print wil uitvoeren moet je gebruik maken van de functie **tf.print()**.\n",
    "\n",
    "Wanneer de graph execution uitgeschakeld wordt door middel van de run_functions_eagerly functie  dan zien we dat de print wel drie keer uitgevoerd wordt.\n",
    "Typisch worden bij graph executions enkel de volgende functies uitgevoerd:\n",
    "* De return value\n",
    "* Functies met well-known side effects zoals\n",
    "    * tf.print\n",
    "    * tf.debugging (asserts bvb)\n",
    "    * aanpassingen van tf.Variable objecten\n",
    "    \n",
    "Dit type gedrag wordt ook non-strict execution genoemd omdat niet alle stappen van het programma doorlopen worden.\n",
    "Dit heeft ook als gevolg dat het kan zijn dat exceptions niet opgeworpen worden als het gebeurt in een functie die niet opgeroepen wordt.\n",
    "**Reken dus niet op het opwerpen van een exception**.\n",
    "Een voorbeeld hiervan kan bestudeerd worden in onderstaande voorbeeld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89ce2f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# eager execution\n",
    "def unused_return_eager(x):\n",
    "    # Get index 1 zou foutmelding moeten opwerpen als de lengte van x 1 is\n",
    "    tf.gather(x, [1]) # unused \n",
    "    return x\n",
    "\n",
    "try:\n",
    "    print(unused_return_eager(tf.constant([0.0])))\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    # All operations are run during eager execution so an error is raised.\n",
    "    print(f'{type(e).__name__}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a9a3d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# graph execution\n",
    "@tf.function\n",
    "def unused_return_graph(x):\n",
    "    tf.gather(x, [1]) # unused\n",
    "    return x\n",
    "\n",
    "# Only needed operations are run during graph execution. The error is not raised.\n",
    "print(unused_return_graph(tf.constant([0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1b32d",
   "metadata": {},
   "source": [
    "**Performantie boost**\n",
    "\n",
    "Gebruik maken van graph execution heeft typisch een performantie winst tot gevolg, afhankelijk van het type van berekening dat je uitvoert.\n",
    "Hieronder staat een voorbeeld waarbij een aantal matrixberekeningen uitgevoerd worden en de uitvoeringstijd voor zowel eager als graph execution wordt berekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7cdd91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: 19.037847800000236\n"
     ]
    }
   ],
   "source": [
    "# eager execution\n",
    "\n",
    "x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)\n",
    "\n",
    "def power(x, y):\n",
    "    result = tf.eye(10, dtype=tf.dtypes.int32)\n",
    "    for _ in range(y):\n",
    "        result = tf.matmul(x, result)\n",
    "    return result\n",
    "\n",
    "print(\"Eager execution:\", timeit.timeit(lambda: power(x, 1000), number=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad3fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph execution: 2.1489677999998094\n"
     ]
    }
   ],
   "source": [
    "# graph execution\n",
    "power_as_graph = tf.function(power)\n",
    "print(\"Graph execution:\", timeit.timeit(lambda: power_as_graph(x, 1000), number=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4a277",
   "metadata": {},
   "source": [
    "Let wel op dat deze performantie boost enkel ervaren wordt wanneer een bepaalde functie meermaals uitgevoerd wordt.\n",
    "De eerste keer dat de functie uitgevoerd wordt is er extra rekentijd nodig om de graph op te bouwen en te optimaliseren (**tracing** genoemd).\n",
    "Deze extra kost op vlak van rekentijd wordt echter snel terugverdiend door de volgende functies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1b866",
   "metadata": {},
   "source": [
    "**Best practices voor graph execution**\n",
    "\n",
    "* **Wissel tussen eager en graph execution** om vast te stellen op welk punt de modes wijzigen.\n",
    "* Maak tf.Variables aan buiten de python functie en wijzig ze binnen de functie. Dit geldt ook voor andere objecten zoals layers, Models, optimizers, ...\n",
    "* Vermijd functies te schrijven die afhangen van standaard python variabelen behalve tf.Variables en Keras objecten.\n",
    "* Geef de voorkeur aan tensors en en tensorflow types als inputs van functies\n",
    "* Plaats zoveel mogelijk bewerkingen in tf.functions om de performantiewinst te optimaliseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5b6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
