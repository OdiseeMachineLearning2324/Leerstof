{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCCMpkHqd_R"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "## Evolutionary algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "# for allowing abstract methodes (closest thing to interface)\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Agent erft over van de klasse ABC\n",
        "class Agent(ABC):\n",
        "\n",
        "    num_parents = 1\n",
        "\n",
        "    @abstractmethod\n",
        "    def _init_weights(self):\n",
        "        # initialisatie met random gewichten\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def copy(self):\n",
        "        # dit is een trucje om nieuwe agents te krijgen van 1\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_action(self, observation=None):\n",
        "        # wat is de actie op basis van mijn huidige state (predict)\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def mutate(parents=None, mutation_rate=None):\n",
        "        # heel domme fit, neem de gewichten over van de parent en muteer ze\n",
        "        # als hij door de mutaties beter scoort dan de parent, zal hij blijven bestaan.\n",
        "        # anders sterft hij uit\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# simulate single training run\n",
        "def simulate_env(env, agent):\n",
        "    observation = env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    # return of cumulatieve reward\n",
        "    ret = 0\n",
        "\n",
        "    while not done:\n",
        "        # Forward pass through the neural network (manueel geschreven)\n",
        "        action = agent.get_action(observation)\n",
        "\n",
        "        # Take the selected action and observe the next state and reward\n",
        "        observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        ret += reward\n",
        "        done = truncated or terminated\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train a reinforcment learning agent\n",
        "def train_agent(env, agent, population_size = 50, mutation_rate=0.4, num_generations = 100, num_episodes=5):\n",
        "    # population_size = aantal giraffen waarmee we werken\n",
        "    # mutation_rate = hoe sterk verschillen de kinderen van hun ouders\n",
        "    # num_generations = aantal epochs\n",
        "    # num_episodes = aantal folds in k-cross validation\n",
        "\n",
        "    # Initialize the population\n",
        "    population = [agent.copy() for _ in range(population_size)]\n",
        "\n",
        "    # number of generations in the algorithm\n",
        "    for generation in range(num_generations):\n",
        "        scores = []\n",
        "\n",
        "        # Evaluate each individual in the population\n",
        "        for current_pop in population:\n",
        "            total_reward = 0\n",
        "\n",
        "            # Run multiple episodes to evaluate an individual's performance\n",
        "            for _ in range(num_episodes):\n",
        "                total_reward += simulate_env(env, current_pop)\n",
        "\n",
        "            # Calculate the average score for this individual\n",
        "            scores.append(total_reward / num_episodes)\n",
        "\n",
        "        # Select the top-performing individuals\n",
        "        # met de standaardwaarden wil deze lijn zeggen: neem de 10 agents met de grootste score\n",
        "        # argsort - sorteer de array maar geef de volgorde van de incides\n",
        "        elite_indices = np.argsort(scores)[-int(0.2 * population_size):]\n",
        "\n",
        "        # Create a new population by mutating and recombining the elite individuals\n",
        "        new_population = [population[elite_indices[-1]]]  # keep best individual\n",
        "\n",
        "        while len(new_population) < population_size:\n",
        "            # dit stukje code is flexibel, het aantal parents kan dus gekozen worden\n",
        "            # kies num_parents willekeurige ouders\n",
        "            indices = np.random.choice(elite_indices, size=agent.num_parents)\n",
        "            # neem de parents uit de population array\n",
        "            parents = [population[index] for index in indices]\n",
        "            # maak een nieuwe populatie\n",
        "            new_population.extend(current_pop.mutate(parents, mutation_rate))\n",
        "\n",
        "        # dood de vorige populatie en vervang het door de nieuwe\n",
        "        population = new_population\n",
        "\n",
        "        # Print the best score in this generation\n",
        "        best_score = max(scores)\n",
        "        if best_score > -140:\n",
        "            mutation_rate = 0.1\n",
        "        print(f\"Generation {generation + 1}: Best Score = {best_score}\")\n",
        "    \n",
        "    # return best individual\n",
        "    return population[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvolutionaryAgent(Agent):\n",
        "    num_parents = 1\n",
        "\n",
        "    # aantal inputs = de grootte van de observation space\n",
        "    # aantal outputs = het aantal mogelijke acties\n",
        "    # hidden_layer_sizes = we werken met een NN -> het aantal neuronen per hidden laag\n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        #print(self.hidden_layers_sizes)\n",
        "        if len(self.hidden_layers_sizes) == 0:\n",
        "            self.weights = [np.random.randn(self.num_inputs, self.outputs)]\n",
        "        else:\n",
        "            self.weights = []\n",
        "\n",
        "            for index, hidden_layer in enumerate(self.hidden_layers_sizes):\n",
        "                if index == 0:\n",
        "                    # eerst stap is van de inputs naar de eerste hidden layer\n",
        "                    self.weights.append(np.random.randn(self.num_inputs, hidden_layer))\n",
        "                else:\n",
        "                    # weights om van de vorige hidden layer naar de huidige te gaan\n",
        "                    self.weights.append(np.random.randn(self.hidden_layers_sizes[index-1], hidden_layer))\n",
        "\n",
        "            # van de laatste hidden layer naar de outputs                \n",
        "            self.weights.append(np.random.randn(hidden_layer, self.num_outputs))\n",
        "\n",
        "        #for layer in self.weights:\n",
        "        #    print(layer.shape)\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = EvolutionaryAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "    # predict de stap van het NN - forward pass\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = observation\n",
        "\n",
        "        for index, hidden_layer in enumerate(self.weights):\n",
        "            if index == len(self.weights)-1:\n",
        "                # lineaire activatiefunctie in de output layer (vorm van regressie)\n",
        "                action_prob = np.dot(action_prob, hidden_layer)\n",
        "            else:\n",
        "                # dit is wat er gebeurd bij de predict in een dense layer met tanh activatiefunctie\n",
        "                action_prob = np.tanh(np.dot(action_prob, hidden_layer))\n",
        "\n",
        "        # return de beste actie\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        # check om te zien of de parent van hetzelfde type is\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], EvolutionaryAgent):\n",
        "            return\n",
        "        \n",
        "        self.weights = [layer + mutation_rate * np.random.randn(*layer.shape) for layer in parents[0].weights]\n",
        "\n",
        "        return [self]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with 1 hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = EvolutionaryAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[32])\n",
        "\n",
        "best_evolutionary_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_evolutionary_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_action_space(agent):\n",
        "\n",
        "    results = []\n",
        "    xs = np.arange(-1.2, 0.6, 0.05)\n",
        "    ys = np.arange(-0.07, 0.07, 0.001)\n",
        "\n",
        "    for x in xs:\n",
        "        tmp = []\n",
        "        for y in ys:\n",
        "            tmp.append(agent.get_action(np.array([x, y])))\n",
        "        results.append(tmp)\n",
        "    results = np.array(results)\n",
        "\n",
        "    plt.imshow(results, cmap='gray', interpolation='none') \n",
        "\n",
        "plot_action_space(best_evolutionary_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Genetic algorithms\n",
        "\n",
        "Dit zijn varianten van evolutionaire algoritmes waarbij gebruik gemaakt wordt van crossover van twee ouders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeneticAgent(Agent):\n",
        "    num_parents = 2\n",
        "    \n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        #print(self.hidden_layers_sizes)\n",
        "        if len(self.hidden_layers_sizes) == 0:\n",
        "            self.weights = [np.random.randn(self.num_inputs, self.outputs)]\n",
        "        else:\n",
        "            self.weights = []\n",
        "\n",
        "            for index, hidden_layer in enumerate(self.hidden_layers_sizes):\n",
        "                if index == 0:\n",
        "                    self.weights.append(np.random.randn(self.num_inputs, hidden_layer))\n",
        "                else:\n",
        "                    self.weights.append(np.random.randn(self.hidden_layers_sizes[index-1], hidden_layer))\n",
        "                \n",
        "            self.weights.append(np.random.randn(hidden_layer, self.num_outputs))\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = GeneticAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = observation\n",
        "\n",
        "        for index, hidden_layer in enumerate(self.weights):\n",
        "            if index == len(self.weights)-1:\n",
        "                # lineaire activatiefunctie\n",
        "                action_prob = np.dot(action_prob, hidden_layer)\n",
        "            else:\n",
        "                # tanh activatiefunctie\n",
        "                action_prob = np.tanh(np.dot(action_prob, hidden_layer))\n",
        "\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], GeneticAgent) and not isinstance(parents[1], GeneticAgent):\n",
        "            return\n",
        "        \n",
        "        # Perform two-point crossover to create a child\n",
        "        # totaal aantal gewichten\n",
        "        total_size = 0\n",
        "        for weight in self.weights:\n",
        "            total_size += weight.shape[0] * weight.shape[1]\n",
        "\n",
        "        # bepaal de cross-over points\n",
        "        crossover_point1 = np.random.randint(0, total_size-2)\n",
        "        crossover_point2 = np.random.randint(crossover_point1 + 1, total_size)\n",
        "        \n",
        "        # flatten weights\n",
        "        weights_parent1 = np.concatenate([w.flatten() for w in parents[0].weights])\n",
        "        weights_parent2 = np.concatenate([w.flatten() for w in parents[1].weights])\n",
        "\n",
        "        # crossover weights\n",
        "        # child 1 en 2 hebben de omgekeerde mutatie\n",
        "        child_weights1 = np.concatenate((weights_parent1[:crossover_point1], weights_parent2[crossover_point1:crossover_point2], weights_parent1[crossover_point2:]), axis=0)\n",
        "        child_weights2 = np.concatenate((weights_parent2[:crossover_point1], weights_parent1[crossover_point1:crossover_point2], weights_parent2[crossover_point2:]), axis=0)\n",
        "\n",
        "        # recreate shapes\n",
        "        stop_point = 0\n",
        "        child1 = parents[0].copy()\n",
        "        child2 = parents[1].copy()\n",
        "\n",
        "        for index, layer in enumerate(self.hidden_layers_sizes):\n",
        "            if index == 0:\n",
        "                size = self.num_inputs * layer\n",
        "            else:\n",
        "                size = self.hidden_layers_sizes[index-1] * layer\n",
        "\n",
        "            target_shape = parents[0].weights[0].shape\n",
        "            child1.weights[index] = child_weights1[stop_point:stop_point+size].reshape(target_shape)\n",
        "            child2.weights[index] = child_weights2[stop_point:stop_point+size].reshape(target_shape)\n",
        "\n",
        "            stop_point += size\n",
        "        \n",
        "        return [child1, child2]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with a hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = GeneticAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[8])\n",
        "\n",
        "best_genetic_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_genetic_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_action_space(best_genetic_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Met tensorflow\n",
        "\n",
        "Een alternatieve manier om dit te doen is door gebruik te maken van een tensorflow model ipv de lagen van het neuraal netwerk zelf te berekenen.\n",
        "\n",
        "Dit kan je doen als volgt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Om tensorflow sneller te laten werken (manueel uitgeschreven code is wel nog sneller)\n",
        "@tf.function\n",
        "def predict(model, x):\n",
        "    return model(x)\n",
        "\n",
        "class GeneticTFAgent(Agent):\n",
        "    num_parents = 2\n",
        "    \n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(self.num_inputs,))\n",
        "        ])\n",
        "        for layer in self.hidden_layers_sizes:\n",
        "            self.model.add(tf.keras.layers.Dense(layer, activation='relu'))\n",
        "\n",
        "        self.model.add(tf.keras.layers.Dense(self.num_outputs, activation='linear'))\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = GeneticTFAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = predict(self.model, observation[np.newaxis, :])\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], GeneticTFAgent) and not isinstance(parents[1], GeneticTFAgent):\n",
        "            return\n",
        "        \n",
        "        child1 = parents[0].copy()\n",
        "        child2 = parents[1].copy()\n",
        "        \n",
        "        # Get the weights of the parents and children\n",
        "        parent1_weights = parents[0].model.get_weights()\n",
        "        parent2_weights = parents[1].model.get_weights()\n",
        "\n",
        "        parent1_weights_flattened = np.concatenate([weights.flatten() for weights in parent1_weights], axis=0)\n",
        "        parent2_weights_flattened = np.concatenate([weights.flatten() for weights in parent2_weights], axis=0)\n",
        "\n",
        "        # Perform two-point crossover on weights\n",
        "        crossover_point1 = np.random.randint(0, len(parent1_weights_flattened)-1)\n",
        "        crossover_point2 = np.random.randint(crossover_point1 + 1, len(parent1_weights_flattened))\n",
        "\n",
        "        # crossover weights\n",
        "        child_weights1 = np.concatenate((parent1_weights_flattened[:crossover_point1], parent2_weights_flattened[crossover_point1:crossover_point2], parent1_weights_flattened[crossover_point2:]), axis=0)\n",
        "        child_weights2 = np.concatenate((parent2_weights_flattened[:crossover_point1], parent1_weights_flattened[crossover_point1:crossover_point2], parent2_weights_flattened[crossover_point2:]), axis=0)\n",
        "        \n",
        "        # mutation\n",
        "        child_weights1 += mutation_rate * np.random.randn(*child_weights1.shape)\n",
        "        child_weights2 += mutation_rate * np.random.randn(*child_weights2.shape)\n",
        "\n",
        "        # recreate shapes\n",
        "        child1_reshaped_weights = []\n",
        "        child2_reshaped_weights = []\n",
        "        idx = 0\n",
        "        for layer in parent1_weights:\n",
        "            size = layer.size\n",
        "            child1_reshaped_weights.append(child_weights1[idx:idx+size].reshape(layer.shape))\n",
        "            child2_reshaped_weights.append(child_weights2[idx:idx+size].reshape(layer.shape))\n",
        "            idx += size\n",
        "\n",
        "        # Set the weights back to the children\n",
        "        child1.model.set_weights(child1_reshaped_weights)\n",
        "        child2.model.set_weights(child2_reshaped_weights)\n",
        "        \n",
        "        return [child1, child2]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with a hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = GeneticTFAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[8])\n",
        "\n",
        "best_genetictf_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_genetictf_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_action_space(best_genetictf_agent)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
