{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCCMpkHqd_R"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "## Evolutionary algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "# for allowing abstract methodes (closest thing to interface)\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Agent erft over van de klasse ABC\n",
        "class Agent(ABC):\n",
        "\n",
        "    num_parents = 1\n",
        "\n",
        "    @abstractmethod\n",
        "    def _init_weights(self):\n",
        "        # initialisatie met random gewichten\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def copy(self):\n",
        "        # dit is een trucje om nieuwe agents te krijgen van 1\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_action(self, observation=None):\n",
        "        # wat is de actie op basis van mijn huidige state (predict)\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def mutate(parents=None, mutation_rate=None):\n",
        "        # heel domme fit, neem de gewichten over van de parent en muteer ze\n",
        "        # als hij door de mutaties beter scoort dan de parent, zal hij blijven bestaan.\n",
        "        # anders sterft hij uit\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# simulate single training run\n",
        "def simulate_env(env, agent):\n",
        "    observation = env.reset()[0]\n",
        "    done = False\n",
        "\n",
        "    # return of cumulatieve reward\n",
        "    ret = 0\n",
        "\n",
        "    while not done:\n",
        "        # Forward pass through the neural network (manueel geschreven)\n",
        "        action = agent.get_action(observation)\n",
        "\n",
        "        # Take the selected action and observe the next state and reward\n",
        "        observation, reward, terminated, truncated, _ = env.step(action)\n",
        "        ret += reward\n",
        "        done = truncated or terminated\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train a reinforcment learning agent\n",
        "def train_agent(env, agent, population_size = 50, mutation_rate=0.4, num_generations = 100, num_episodes=5):\n",
        "    # population_size = aantal giraffen waarmee we werken\n",
        "    # mutation_rate = hoe sterk verschillen de kinderen van hun ouders\n",
        "    # num_generations = aantal epochs\n",
        "    # num_episodes = aantal folds in k-cross validation\n",
        "\n",
        "    # Initialize the population\n",
        "    population = [agent.copy() for _ in range(population_size)]\n",
        "\n",
        "    # number of generations in the algorithm\n",
        "    for generation in range(num_generations):\n",
        "        scores = []\n",
        "\n",
        "        # Evaluate each individual in the population\n",
        "        for current_pop in population:\n",
        "            total_reward = 0\n",
        "\n",
        "            # Run multiple episodes to evaluate an individual's performance\n",
        "            for _ in range(num_episodes):\n",
        "                total_reward += simulate_env(env, current_pop)\n",
        "\n",
        "            # Calculate the average score for this individual\n",
        "            scores.append(total_reward / num_episodes)\n",
        "\n",
        "        # Select the top-performing individuals\n",
        "        # met de standaardwaarden wil deze lijn zeggen: neem de 10 agents met de grootste score\n",
        "        # argsort - sorteer de array maar geef de volgorde van de incides\n",
        "        elite_indices = np.argsort(scores)[-int(0.2 * population_size):]\n",
        "\n",
        "        # Create a new population by mutating and recombining the elite individuals\n",
        "        new_population = [population[elite_indices[-1]]]  # keep best individual\n",
        "\n",
        "        while len(new_population) < population_size:\n",
        "            # dit stukje code is flexibel, het aantal parents kan dus gekozen worden\n",
        "            # kies num_parents willekeurige ouders\n",
        "            indices = np.random.choice(elite_indices, size=agent.num_parents)\n",
        "            # neem de parents uit de population array\n",
        "            parents = [population[index] for index in indices]\n",
        "            # maak een nieuwe populatie\n",
        "            new_population.extend(current_pop.mutate(parents, mutation_rate))\n",
        "\n",
        "        # dood de vorige populatie en vervang het door de nieuwe\n",
        "        population = new_population\n",
        "\n",
        "        # Print the best score in this generation\n",
        "        best_score = max(scores)\n",
        "        if best_score > -140:\n",
        "            mutation_rate = 0.1\n",
        "        print(f\"Generation {generation + 1}: Best Score = {best_score}\")\n",
        "    \n",
        "    # return best individual\n",
        "    return population[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EvolutionaryAgent(Agent):\n",
        "    num_parents = 1\n",
        "\n",
        "    # aantal inputs = de grootte van de observation space\n",
        "    # aantal outputs = het aantal mogelijke acties\n",
        "    # hidden_layer_sizes = we werken met een NN -> het aantal neuronen per hidden laag\n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        #print(self.hidden_layers_sizes)\n",
        "        if len(self.hidden_layers_sizes) == 0:\n",
        "            self.weights = [np.random.randn(self.num_inputs, self.outputs)]\n",
        "        else:\n",
        "            self.weights = []\n",
        "\n",
        "            for index, hidden_layer in enumerate(self.hidden_layers_sizes):\n",
        "                if index == 0:\n",
        "                    # eerst stap is van de inputs naar de eerste hidden layer\n",
        "                    self.weights.append(np.random.randn(self.num_inputs, hidden_layer))\n",
        "                else:\n",
        "                    # weights om van de vorige hidden layer naar de huidige te gaan\n",
        "                    self.weights.append(np.random.randn(self.hidden_layers_sizes[index-1], hidden_layer))\n",
        "\n",
        "            # van de laatste hidden layer naar de outputs                \n",
        "            self.weights.append(np.random.randn(hidden_layer, self.num_outputs))\n",
        "\n",
        "        #for layer in self.weights:\n",
        "        #    print(layer.shape)\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = EvolutionaryAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "    # predict de stap van het NN - forward pass\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = observation\n",
        "\n",
        "        for index, hidden_layer in enumerate(self.weights):\n",
        "            if index == len(self.weights)-1:\n",
        "                # lineaire activatiefunctie in de output layer (vorm van regressie)\n",
        "                action_prob = np.dot(action_prob, hidden_layer)\n",
        "            else:\n",
        "                # dit is wat er gebeurd bij de predict in een dense layer met tanh activatiefunctie\n",
        "                action_prob = np.tanh(np.dot(action_prob, hidden_layer))\n",
        "\n",
        "        # return de beste actie\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        # check om te zien of de parent van hetzelfde type is\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], EvolutionaryAgent):\n",
        "            return\n",
        "        \n",
        "        self.weights = [layer + mutation_rate * np.random.randn(*layer.shape) for layer in parents[0].weights]\n",
        "\n",
        "        return [self]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 1: Best Score = -200.0\n",
            "Generation 2: Best Score = -200.0\n",
            "Generation 3: Best Score = -200.0\n",
            "Generation 4: Best Score = -200.0\n",
            "Generation 5: Best Score = -200.0\n",
            "Generation 6: Best Score = -200.0\n",
            "Generation 7: Best Score = -200.0\n",
            "Generation 8: Best Score = -200.0\n",
            "Generation 9: Best Score = -200.0\n",
            "Generation 10: Best Score = -200.0\n",
            "Generation 11: Best Score = -200.0\n",
            "Generation 12: Best Score = -200.0\n",
            "Generation 13: Best Score = -200.0\n",
            "Generation 14: Best Score = -200.0\n",
            "Generation 15: Best Score = -200.0\n",
            "Generation 16: Best Score = -200.0\n",
            "Generation 17: Best Score = -200.0\n",
            "Generation 18: Best Score = -200.0\n",
            "Generation 19: Best Score = -200.0\n",
            "Generation 20: Best Score = -200.0\n",
            "Generation 21: Best Score = -200.0\n",
            "Generation 22: Best Score = -200.0\n",
            "Generation 23: Best Score = -200.0\n",
            "Generation 24: Best Score = -200.0\n",
            "Generation 25: Best Score = -200.0\n",
            "Generation 26: Best Score = -200.0\n",
            "Generation 27: Best Score = -200.0\n",
            "Generation 28: Best Score = -200.0\n",
            "Generation 29: Best Score = -200.0\n",
            "Generation 30: Best Score = -200.0\n",
            "Generation 31: Best Score = -200.0\n",
            "Generation 32: Best Score = -200.0\n",
            "Generation 33: Best Score = -200.0\n",
            "Generation 34: Best Score = -200.0\n",
            "Generation 35: Best Score = -200.0\n",
            "Generation 36: Best Score = -200.0\n",
            "Generation 37: Best Score = -200.0\n",
            "Generation 38: Best Score = -200.0\n",
            "Generation 39: Best Score = -200.0\n",
            "Generation 40: Best Score = -200.0\n",
            "Generation 41: Best Score = -200.0\n",
            "Generation 42: Best Score = -200.0\n",
            "Generation 43: Best Score = -200.0\n",
            "Generation 44: Best Score = -200.0\n",
            "Generation 45: Best Score = -200.0\n",
            "Generation 46: Best Score = -200.0\n",
            "Generation 47: Best Score = -200.0\n",
            "Generation 48: Best Score = -200.0\n",
            "Generation 49: Best Score = -200.0\n",
            "Generation 50: Best Score = -200.0\n",
            "Generation 51: Best Score = -200.0\n",
            "Generation 52: Best Score = -200.0\n",
            "Generation 53: Best Score = -200.0\n",
            "Generation 54: Best Score = -200.0\n",
            "Generation 55: Best Score = -200.0\n",
            "Generation 56: Best Score = -200.0\n",
            "Generation 57: Best Score = -200.0\n",
            "Generation 58: Best Score = -200.0\n",
            "Generation 59: Best Score = -200.0\n",
            "Generation 60: Best Score = -200.0\n",
            "Generation 61: Best Score = -200.0\n",
            "Generation 62: Best Score = -200.0\n",
            "Generation 63: Best Score = -200.0\n",
            "Generation 64: Best Score = -200.0\n",
            "Generation 65: Best Score = -200.0\n",
            "Generation 66: Best Score = -200.0\n",
            "Generation 67: Best Score = -200.0\n",
            "Generation 68: Best Score = -200.0\n",
            "Generation 69: Best Score = -200.0\n",
            "Generation 70: Best Score = -200.0\n",
            "Generation 71: Best Score = -200.0\n",
            "Generation 72: Best Score = -200.0\n",
            "Generation 73: Best Score = -200.0\n",
            "Generation 74: Best Score = -200.0\n",
            "Generation 75: Best Score = -200.0\n",
            "Generation 76: Best Score = -200.0\n",
            "Generation 77: Best Score = -200.0\n",
            "Generation 78: Best Score = -200.0\n",
            "Generation 79: Best Score = -200.0\n",
            "Generation 80: Best Score = -200.0\n",
            "Generation 81: Best Score = -200.0\n",
            "Generation 82: Best Score = -200.0\n",
            "Generation 83: Best Score = -200.0\n",
            "Generation 84: Best Score = -200.0\n",
            "Generation 85: Best Score = -200.0\n",
            "Generation 86: Best Score = -200.0\n",
            "Generation 87: Best Score = -200.0\n",
            "Generation 88: Best Score = -200.0\n",
            "Generation 89: Best Score = -200.0\n",
            "Generation 90: Best Score = -200.0\n",
            "Generation 91: Best Score = -200.0\n",
            "Generation 92: Best Score = -200.0\n",
            "Generation 93: Best Score = -200.0\n",
            "Generation 94: Best Score = -200.0\n",
            "Generation 95: Best Score = -200.0\n",
            "Generation 96: Best Score = -200.0\n",
            "Generation 97: Best Score = -200.0\n",
            "Generation 98: Best Score = -200.0\n",
            "Generation 99: Best Score = -200.0\n",
            "Generation 100: Best Score = -200.0\n"
          ]
        }
      ],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with 1 hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = EvolutionaryAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[32])\n",
        "\n",
        "best_evolutionary_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Individual Score: -200.0\n",
            "Best Individual Score: -200.0\n",
            "Best Individual Score: -200.0\n",
            "Best Individual Score: -200.0\n",
            "Best Individual Score: -200.0\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_evolutionary_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_action_space(agent):\n",
        "\n",
        "    results = []\n",
        "    xs = np.arange(-1.2, 0.6, 0.05)\n",
        "    ys = np.arange(-0.07, 0.07, 0.001)\n",
        "\n",
        "    for x in xs:\n",
        "        tmp = []\n",
        "        for y in ys:\n",
        "            tmp.append(agent.get_action(np.array([x, y])))\n",
        "        results.append(tmp)\n",
        "    results = np.array(results)\n",
        "\n",
        "    plt.imshow(results, cmap='gray', interpolation='none') \n",
        "\n",
        "plot_action_space(best_evolutionary_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Genetic algorithms\n",
        "\n",
        "Dit zijn varianten van evolutionaire algoritmes waarbij gebruik gemaakt wordt van crossover van twee ouders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeneticAgent(Agent):\n",
        "    num_parents = 2\n",
        "    \n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        #print(self.hidden_layers_sizes)\n",
        "        if len(self.hidden_layers_sizes) == 0:\n",
        "            self.weights = [np.random.randn(self.num_inputs, self.outputs)]\n",
        "        else:\n",
        "            self.weights = []\n",
        "\n",
        "            for index, hidden_layer in enumerate(self.hidden_layers_sizes):\n",
        "                if index == 0:\n",
        "                    self.weights.append(np.random.randn(self.num_inputs, hidden_layer))\n",
        "                else:\n",
        "                    self.weights.append(np.random.randn(self.hidden_layers_sizes[index-1], hidden_layer))\n",
        "                \n",
        "            self.weights.append(np.random.randn(hidden_layer, self.num_outputs))\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = GeneticAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = observation\n",
        "\n",
        "        for index, hidden_layer in enumerate(self.weights):\n",
        "            if index == len(self.weights)-1:\n",
        "                # lineaire activatiefunctie\n",
        "                action_prob = np.dot(action_prob, hidden_layer)\n",
        "            else:\n",
        "                # tanh activatiefunctie\n",
        "                action_prob = np.tanh(np.dot(action_prob, hidden_layer))\n",
        "\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], GeneticAgent) and not isinstance(parents[1], GeneticAgent):\n",
        "            return\n",
        "        \n",
        "        # Perform two-point crossover to create a child\n",
        "        # totaal aantal gewichten\n",
        "        total_size = 0\n",
        "        for weight in self.weights:\n",
        "            total_size += weight.shape[0] * weight.shape[1]\n",
        "\n",
        "        # bepaal de cross-over points\n",
        "        crossover_point1 = np.random.randint(0, total_size-2)\n",
        "        crossover_point2 = np.random.randint(crossover_point1 + 1, total_size)\n",
        "        \n",
        "        # flatten weights\n",
        "        weights_parent1 = np.concatenate([w.flatten() for w in parents[0].weights])\n",
        "        weights_parent2 = np.concatenate([w.flatten() for w in parents[1].weights])\n",
        "\n",
        "        # crossover weights\n",
        "        # child 1 en 2 hebben de omgekeerde mutatie\n",
        "        child_weights1 = np.concatenate((weights_parent1[:crossover_point1], weights_parent2[crossover_point1:crossover_point2], weights_parent1[crossover_point2:]), axis=0)\n",
        "        child_weights2 = np.concatenate((weights_parent2[:crossover_point1], weights_parent1[crossover_point1:crossover_point2], weights_parent2[crossover_point2:]), axis=0)\n",
        "\n",
        "        # recreate shapes\n",
        "        stop_point = 0\n",
        "        child1 = parents[0].copy()\n",
        "        child2 = parents[1].copy()\n",
        "\n",
        "        for index, layer in enumerate(self.hidden_layers_sizes):\n",
        "            if index == 0:\n",
        "                size = self.num_inputs * layer\n",
        "            else:\n",
        "                size = self.hidden_layers_sizes[index-1] * layer\n",
        "\n",
        "            target_shape = parents[0].weights[0].shape\n",
        "            child1.weights[index] = child_weights1[stop_point:stop_point+size].reshape(target_shape)\n",
        "            child2.weights[index] = child_weights2[stop_point:stop_point+size].reshape(target_shape)\n",
        "\n",
        "            stop_point += size\n",
        "        \n",
        "        return [child1, child2]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 1: Best Score = -136.6\n",
            "Generation 2: Best Score = -145.2\n",
            "Generation 3: Best Score = -148.2\n",
            "Generation 4: Best Score = -145.2\n",
            "Generation 5: Best Score = -146.6\n",
            "Generation 6: Best Score = -149.6\n",
            "Generation 7: Best Score = -135.6\n",
            "Generation 8: Best Score = -152.4\n",
            "Generation 9: Best Score = -166.8\n",
            "Generation 10: Best Score = -154.4\n",
            "Generation 11: Best Score = -165.6\n",
            "Generation 12: Best Score = -135.0\n",
            "Generation 13: Best Score = -164.8\n",
            "Generation 14: Best Score = -165.8\n",
            "Generation 15: Best Score = -161.4\n",
            "Generation 16: Best Score = -131.4\n",
            "Generation 17: Best Score = -131.4\n",
            "Generation 18: Best Score = -114.8\n",
            "Generation 19: Best Score = -131.2\n",
            "Generation 20: Best Score = -115.2\n",
            "Generation 21: Best Score = -123.6\n",
            "Generation 22: Best Score = -139.6\n",
            "Generation 23: Best Score = -115.2\n",
            "Generation 24: Best Score = -114.8\n",
            "Generation 25: Best Score = -115.6\n",
            "Generation 26: Best Score = -115.4\n",
            "Generation 27: Best Score = -122.2\n",
            "Generation 28: Best Score = -139.6\n",
            "Generation 29: Best Score = -132.2\n",
            "Generation 30: Best Score = -130.6\n",
            "Generation 31: Best Score = -114.8\n",
            "Generation 32: Best Score = -123.6\n",
            "Generation 33: Best Score = -114.8\n",
            "Generation 34: Best Score = -131.4\n",
            "Generation 35: Best Score = -131.8\n",
            "Generation 36: Best Score = -132.2\n",
            "Generation 37: Best Score = -123.8\n",
            "Generation 38: Best Score = -115.4\n",
            "Generation 39: Best Score = -114.8\n",
            "Generation 40: Best Score = -124.2\n",
            "Generation 41: Best Score = -123.0\n",
            "Generation 42: Best Score = -140.2\n",
            "Generation 43: Best Score = -123.6\n",
            "Generation 44: Best Score = -115.2\n",
            "Generation 45: Best Score = -123.4\n",
            "Generation 46: Best Score = -122.8\n",
            "Generation 47: Best Score = -115.2\n",
            "Generation 48: Best Score = -123.2\n",
            "Generation 49: Best Score = -115.0\n",
            "Generation 50: Best Score = -115.6\n",
            "Generation 51: Best Score = -132.0\n",
            "Generation 52: Best Score = -115.2\n",
            "Generation 53: Best Score = -114.8\n",
            "Generation 54: Best Score = -114.6\n",
            "Generation 55: Best Score = -123.6\n",
            "Generation 56: Best Score = -123.4\n",
            "Generation 57: Best Score = -115.0\n",
            "Generation 58: Best Score = -123.2\n",
            "Generation 59: Best Score = -123.6\n",
            "Generation 60: Best Score = -131.8\n",
            "Generation 61: Best Score = -139.2\n",
            "Generation 62: Best Score = -123.4\n",
            "Generation 63: Best Score = -115.2\n",
            "Generation 64: Best Score = -122.8\n",
            "Generation 65: Best Score = -123.2\n",
            "Generation 66: Best Score = -115.0\n",
            "Generation 67: Best Score = -115.4\n",
            "Generation 68: Best Score = -115.0\n",
            "Generation 69: Best Score = -131.8\n",
            "Generation 70: Best Score = -122.2\n",
            "Generation 71: Best Score = -130.4\n",
            "Generation 72: Best Score = -122.8\n",
            "Generation 73: Best Score = -114.8\n",
            "Generation 74: Best Score = -123.8\n",
            "Generation 75: Best Score = -123.2\n",
            "Generation 76: Best Score = -115.0\n",
            "Generation 77: Best Score = -115.4\n",
            "Generation 78: Best Score = -123.4\n",
            "Generation 79: Best Score = -115.2\n",
            "Generation 80: Best Score = -123.0\n",
            "Generation 81: Best Score = -121.4\n",
            "Generation 82: Best Score = -122.8\n",
            "Generation 83: Best Score = -121.6\n",
            "Generation 84: Best Score = -133.8\n",
            "Generation 85: Best Score = -127.8\n",
            "Generation 86: Best Score = -127.6\n",
            "Generation 87: Best Score = -145.4\n",
            "Generation 88: Best Score = -140.0\n",
            "Generation 89: Best Score = -129.2\n",
            "Generation 90: Best Score = -140.8\n",
            "Generation 91: Best Score = -128.4\n",
            "Generation 92: Best Score = -133.0\n",
            "Generation 93: Best Score = -135.8\n",
            "Generation 94: Best Score = -155.2\n",
            "Generation 95: Best Score = -166.2\n",
            "Generation 96: Best Score = -160.8\n",
            "Generation 97: Best Score = -139.2\n",
            "Generation 98: Best Score = -149.0\n",
            "Generation 99: Best Score = -167.8\n",
            "Generation 100: Best Score = -171.0\n"
          ]
        }
      ],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with a hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = GeneticAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[8])\n",
        "\n",
        "best_genetic_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Individual Score: -115.0\n",
            "Best Individual Score: -122.0\n",
            "Best Individual Score: -115.0\n",
            "Best Individual Score: -145.0\n",
            "Best Individual Score: -145.0\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_genetic_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_action_space(best_genetic_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Met tensorflow\n",
        "\n",
        "Een alternatieve manier om dit te doen is door gebruik te maken van een tensorflow model ipv de lagen van het neuraal netwerk zelf te berekenen.\n",
        "\n",
        "Dit kan je doen als volgt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class GeneticTFAgent(Agent):\n",
        "    num_parents = 2\n",
        "    \n",
        "    def __init__(self, num_inputs=1, num_outputs=1, hidden_layer_sizes=[]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.hidden_layers_sizes = hidden_layer_sizes\n",
        "        \n",
        "        self._init_weights()\n",
        "        \n",
        "    def _init_weights(self):\n",
        "\n",
        "        self.model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Input(shape=(self.num_inputs,))\n",
        "        ])\n",
        "        for layer in self.hidden_layers_sizes:\n",
        "            self.model.add(tf.keras.layers.Dense(layer, activation='relu'))\n",
        "\n",
        "        self.model.add(tf.keras.layers.Dense(self.num_outputs, activation='linear'))\n",
        "                \n",
        "    def copy(self):\n",
        "        agent = GeneticTFAgent(self.num_inputs, self.num_outputs, self.hidden_layers_sizes)\n",
        "        agent._init_weights()\n",
        "\n",
        "        return agent\n",
        "        \n",
        "\n",
        "    def get_action(self, observation=None):\n",
        "\n",
        "        action_prob = self.model(observation[np.newaxis, :])\n",
        "        return np.argmax(action_prob)\n",
        "\n",
        "    def mutate(self, parents=None, mutation_rate=None):\n",
        "        if not isinstance(parents, list) and not isinstance(parents[0], GeneticTFAgent) and not isinstance(parents[1], GeneticTFAgent):\n",
        "            return\n",
        "        \n",
        "        child1 = parents[0].copy()\n",
        "        child2 = parents[1].copy()\n",
        "        \n",
        "        # Get the weights of the parents and children\n",
        "        parent1_weights = parents[0].model.get_weights()\n",
        "        parent2_weights = parents[1].model.get_weights()\n",
        "\n",
        "        parent1_weights_flattened = np.concatenate([weights.flatten() for weights in parent1_weights], axis=0)\n",
        "        parent2_weights_flattened = np.concatenate([weights.flatten() for weights in parent2_weights], axis=0)\n",
        "\n",
        "        # Perform two-point crossover on weights\n",
        "        crossover_point1 = np.random.randint(0, len(parent1_weights_flattened)-1)\n",
        "        crossover_point2 = np.random.randint(crossover_point1 + 1, len(parent1_weights_flattened))\n",
        "\n",
        "        # crossover weights\n",
        "        child_weights1 = np.concatenate((parent1_weights_flattened[:crossover_point1], parent2_weights_flattened[crossover_point1:crossover_point2], parent1_weights_flattened[crossover_point2:]), axis=0)\n",
        "        child_weights2 = np.concatenate((parent2_weights_flattened[:crossover_point1], parent1_weights_flattened[crossover_point1:crossover_point2], parent2_weights_flattened[crossover_point2:]), axis=0)\n",
        "        \n",
        "        # mutation\n",
        "        child_weights1 += mutation_rate * np.random.randn(*child_weights1.shape)\n",
        "        child_weights2 += mutation_rate * np.random.randn(*child_weights2.shape)\n",
        "\n",
        "        # recreate shapes\n",
        "        child1_reshaped_weights = []\n",
        "        child2_reshaped_weights = []\n",
        "        idx = 0\n",
        "        for layer in parent1_weights:\n",
        "            size = layer.size\n",
        "            child1_reshaped_weights.append(child_weights1[idx:idx+size].reshape(layer.shape))\n",
        "            child2_reshaped_weights.append(child_weights2[idx:idx+size].reshape(layer.shape))\n",
        "            idx += size\n",
        "\n",
        "        # Set the weights back to the children\n",
        "        child1.model.set_weights(child1_reshaped_weights)\n",
        "        child2.model.set_weights(child2_reshaped_weights)\n",
        "        \n",
        "        return [child1, child2]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 1: Best Score = -181.2\n",
            "Generation 2: Best Score = -181.2\n",
            "Generation 3: Best Score = -182.6\n",
            "Generation 4: Best Score = -197.2\n",
            "Generation 5: Best Score = -183.6\n",
            "Generation 6: Best Score = -189.4\n",
            "Generation 7: Best Score = -176.4\n",
            "Generation 8: Best Score = -181.4\n",
            "Generation 9: Best Score = -190.8\n",
            "Generation 10: Best Score = -182.2\n",
            "Generation 11: Best Score = -193.4\n"
          ]
        }
      ],
      "source": [
        "# Define the MountainCar environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "population_size = 100\n",
        "mutation_rate = 0.4\n",
        "num_generations = 100\n",
        "num_episodes = 5\n",
        "\n",
        "# RL agent with internally a NN with a hidden layer of 8 neurons\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "agent = GeneticTFAgent(num_inputs=input_size, num_outputs=output_size, hidden_layer_sizes=[8])\n",
        "\n",
        "best_genetictf_agent = train_agent(env, agent, population_size=population_size, mutation_rate=mutation_rate, num_generations=num_generations, num_episodes=num_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Individual Score: -115.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMountainCar-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43msimulate_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_genetic_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Individual Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36msimulate_env\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     14\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(observation)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Take the selected action and observe the next state and reward\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m ret \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     19\u001b[0m done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated\n",
            "File \u001b[1;32mc:\\Users\\jens.baetens3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
            "File \u001b[1;32mc:\\Users\\jens.baetens3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jens.baetens3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jens.baetens3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:148\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m (position, velocity)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
            "File \u001b[1;32mc:\\Users\\jens.baetens3\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:266\u001b[0m, in \u001b[0;36mMountainCarEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Evaluate the best individual\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
        "\n",
        "for episode in range(5):\n",
        "    score = simulate_env(env, best_genetictf_agent)\n",
        "    print(f\"Best Individual Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_action_space(best_genetictf_agent)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
