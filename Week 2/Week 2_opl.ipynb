{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwBCE43Cv3PH"
   },
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQB7yiF6v9GR"
   },
   "source": [
    "## Inladen\n",
    "\n",
    "### Standaard datasets\n",
    "\n",
    "Via tf.keras.datasets zijn de volgende datasets beschikbaar:\n",
    "* boston-housing\n",
    "* cifar-10\n",
    "* cifar-100\n",
    "* mnist\n",
    "* fashion-mnist\n",
    "* imdb\n",
    "* reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmyEaf4Awl2v"
   },
   "source": [
    "### Gestructureerde data\n",
    "\n",
    "De volgende manieren zijn voorbeelden van hoe gestructureerde data kan ingeladen worden:\n",
    "* Omgezet naar tensors via pandas, hierbij gebeurt preprocessing via pandas of scikit-learn\n",
    "    * Goed als je nog EDA moet doen. Hierbij kan je namelijk flexibeler werken\n",
    "    * Heeft wel als voorwaarde dat je je data volledig in memory kan inladen\n",
    "* Rechtstreeks inlezen via tf.data. Het voordeel hierbij is dat je direct heel wat functionaliteit hebt om het trainen efficienter te maken.\n",
    "    * Meer functionaliteit\n",
    "    * Data in stukjes ingeladen\n",
    "    * Preprocessing moeilijker door dit in stukjes\n",
    "    * Gebruik bvb map-functie om gemiddelden / std te berekenen van numerieke kolommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methode 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "\n",
    "df = pd.read_csv(titanic_file)\n",
    "display(df.head())\n",
    "\n",
    "# split inputs en outputs\n",
    "titanic_features = df.copy()\n",
    "titanic_labels = titanic_features.pop('survived')\n",
    "titanic_features_dict = {name: np.array(value) \n",
    "                         for name, value in titanic_features.items()}\n",
    "\n",
    "inputs = {}\n",
    "for name, column in titanic_features.items():\n",
    "  dtype = column.dtype\n",
    "  if dtype == object:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# normaliseer de numerieke kolommen\n",
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}\n",
    "\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))         # groepeer de numerieke kolommen\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(titanic_features[numeric_inputs.keys()]))   # bereken gemiddelde en std per kolom \n",
    "all_numeric_inputs = norm(x)                                    # normaliseer de numerieke kolommen\n",
    "preprocessed_inputs = [all_numeric_inputs]\n",
    "\n",
    "# categorieke kolommen\n",
    "for name, input in inputs.items():\n",
    "  if input.dtype == tf.float32:\n",
    "    continue\n",
    "\n",
    "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))  # tekst -> ordinal encoding\n",
    "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())      # ordinal encoding -> one-hot encoding\n",
    "\n",
    "  x = lookup(input)\n",
    "  x = one_hot(x)\n",
    "  preprocessed_inputs.append(x)\n",
    "\n",
    "# voeg alles samen tot 1 preprocessing stap die in zijn geheel uitgevoerd kan worden\n",
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "\n",
    "# teken de graaf van dit preprocessing model\n",
    "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methode 2\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, \n",
    "    batch_size=4, # batch size 4 is extremely small (use larger sizes in real situations (32 or larger))\n",
    "#    select_columns=['class', 'fare', 'survived'],  # subset of columns\n",
    "    label_name=\"survived\",     # this is for splitting the batches in inputs en outputs\n",
    "    num_epochs=1) # only iterate once over the dataset (default it gets repeated)\n",
    "\n",
    "# optionally split in train/test (see tf.keras.utils.split_dataset())\n",
    "\n",
    "[(train_features, label_batch)] = titanic_batches.take(1)\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('A batch of ages:', train_features['age'])\n",
    "print('A batch of targets:', label_batch )\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for the feature.\n",
    "  normalizer = layers.Normalization(axis=None)\n",
    "  # Prepare a Dataset that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "\n",
    "all_inputs = []  \n",
    "encoded_features = []\n",
    "\n",
    "# Numerical features.\n",
    "for header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, titanic_batches)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)                      # important so that the order of the inputs = the order they are concatenated/processed\n",
    "  encoded_features.append(encoded_numeric_col)        # all layers for performng processing\n",
    "\n",
    "categorical_cols = ['sex', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "for header in categorical_cols:\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(name=header,\n",
    "                                               dataset=titanic_batches,\n",
    "                                               dtype='string',\n",
    "                                               max_tokens=None)\n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "# construct the model\n",
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, all_features)\n",
    "\n",
    "tf.keras.utils.plot_model(model = model , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bovenstaande code wordt eerst steeds een lookup/normalisatie gedaan en dan geconcateneerd.\n",
    "Dit is goed op voor dit relatief klein aantal features maar kan inefficient zijn bij meer features.\n",
    "Om eerst gelijksoortige features te groeperen en dan met 1 laag te verwerken kan onderstaande functie gebruikt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# deze funtie kan gebruikt worden om concatenatie van meerdere kolommen van hetzelfde type te doen\n",
    "# dit is efficienter omdat zo 1 laag gebruikt kan worden ipv meerdere.\n",
    "def stack_dict(inputs, dtype=tf.float32, fun=tf.stack):\n",
    "  values = []\n",
    "  for key in sorted(inputs.keys()):\n",
    "    values.append(tf.cast(inputs[key], dtype))\n",
    "\n",
    "  return fun(values, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model opstellen\n",
    "\n",
    "Er zijn twee methodes om een eenvoudig (zonder lussen) neuraal netwerk op te stellen:\n",
    "* Sequentieel model\n",
    "* Functional API\n",
    "\n",
    "Deze methodes kunnen gecombineerd worden om een model op te bouwen maar voor duidelijkheid en leesbaarheid raad ik aan in je code te kiezen voor 1 methode.\n",
    "\n",
    "**Sequentieel model**\n",
    "\n",
    "De eerste mogelijkheid om een model op te bouwen is door gebruik te maken van een sequentieel model.\n",
    "Hierbij wordt een lijst opgesteld met alle lagen die achter elkaar uitgevoerd moeten worden.\n",
    "Hierbij is het belangrijk om te realiseren dat de output van een laag steeds de input van de volgende laag is.\n",
    "Output van vorige lagen kan dus niet gebruikt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequentieel model\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential([\n",
    "  layers.Dense(128, activation='relu', input_shape=(None, 28)),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional API**\n",
    "\n",
    "De tweede mogelijkheid is om gebruik te maken van de functional API.\n",
    "Hierbij worden de lagen aan elkaar gekoppeld door middel van functie-oproepen.\n",
    "Dit maakt het mogelijk om een flexibelere netwerkarchitectuur te bekomen, namelijk kunnen er zo meerdere inputs/outputs, gedeelde lagen en een niet-lineaire topologie gebruikt worden.\n",
    "Hieronder staat een code-voorbeeld hiervan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Dense(128, activation='relu')(preprocessed_inputs_cat)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, x)\n",
    "\n",
    "tf.keras.utils.plot_model(model = model , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainen van een model\n",
    "\n",
    "Na het opstellen van het model, moeten de interne gewichten nog getrained worden. \n",
    "Hiervoor moet eerst het model gecompileerd worden. \n",
    "Dit gebeurt door de compile() functie waarbij de volgende belangrijke parameters ingesteld moeten worden:\n",
    "* De gekozen lossfunctie: dit is de functie waarvoor de gewichten geoptimaliseerd worden.\n",
    "* De optimizer: Dit is hoe de learning rate aangepast wordt\n",
    "* De metrieken: Dit is welke metrieken mee opgenomen worden voor evaluatie (niet rechtstreeks een impact op het leerproces)\n",
    "\n",
    "Na compilatie is het mogelijk om met de summary() functie een overzicht te krijgen van het model.\n",
    "Dit kan goed gebruikt worden om de output dimensies van de verschillende lagen te bekijken en het aantal te trainen parameters.\n",
    "\n",
    "Nu kunnen de gewichten getrained worden door middel van de .fit() functie.\n",
    "Naast de input en gewenste output mee te geven moeten ook de volgende waarden ingesteld worden:\n",
    "* epochs: Het aantal epochs of het aantal keer dat de volledige dataset gebruikt wordt voor training\n",
    "* batch_dize: De grootte van de groepen die doorheen het model gaan voor de gewichten aangepast worden.\n",
    "* Validation_size: Percent van de data dat gebruikt wordt voor validatie\n",
    "\n",
    "Het aantal epochs moet groter zijn als er meer gewichten te trainen zijn en als er minder data beschikbaar is.\n",
    "Dit is vooral het geval bij niet-gestructureerde data en minder bij gestructureerde.\n",
    "De batch_size heeft geen grote impact op de behaalde resultaten maar wel op de snelheid waarmee deze gehaald worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainen\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=titanic_features_dict, y=titanic_labels, batch_size=32, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluatie\n",
    "\n",
    "In de laatste stap moet het model nog gevalueerd worden.\n",
    "Dit kan door middel van de historiek die door het trainingsproces teruggegeven wordt.\n",
    "Hieruit kan heel wat data gehaald worden maar kunnen ook plotjes getekend worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Een andere belangrijke optie van het leerproces die nog niet vermeld is, is het toevoegen van callbacks.\n",
    "Dit is extra functionaliteit die kan uitgevoerd worden na het uitvoeren van elke batch, elke epoch, elke trainingsrun, elke voorspelling, ...\n",
    "Deze callbacks kunnen gebruikt worden om bijvoorbeeld het leeproces te onderbreken wanneer overfitting gedetecteerd wordt, backups gemaakt worden na een aantal epochs in het geval er fouten zouden optreden of voor logging naar tensorboard.\n",
    "Hieronder zie je een voorbeeld van dit laatste.\n",
    "\n",
    "Start tijdens onderstaande code een tensorboard in cli (in de huidige directory) met het volgende commando\n",
    "\n",
    "````\n",
    "    tensorboard --logdir logs\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainen\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=\"./logs\",\n",
    "        histogram_freq=1,       # How often to log histogram visualizationss\n",
    "        update_freq=\"epoch\",\n",
    "        profile_batch=10\n",
    "    ) \n",
    "]\n",
    "\n",
    "history = model.fit(x=titanic_features_dict, y=titanic_labels, callbacks=callbacks, batch_size=32, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas_dataframe.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
