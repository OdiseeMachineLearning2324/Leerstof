{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCCMpkHqd_R"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "Reinforcement learning komt uit de studie van Markov Chains of Processen voor.\n",
        "Dit is een random opeenvolging van states waarbij elke transisitie een mogelijke kans heeft.\n",
        "Door een reward te koppelen aan elke state waarin je komt kan je een functie opstellen die de de totale reward maximaliseert.\n",
        "Dit is het basisidee achter reinforcement learning.\n",
        "\n",
        "Een aantal belangrijke termen/concepten hierbij zijn:\n",
        "* De agent\n",
        "* Het environment\n",
        "* De state space\n",
        "* De action space\n",
        "* De reward en return\n",
        "* Exploration vs exploitation\n",
        "\n",
        "## Q-learning\n",
        "\n",
        "Een eerste algoritme dat we bekijken voor reinforcement learning uit te voeren is Q-learning.\n",
        "Dit algoritme maakt gebruik van de Q-functie of action-value function.\n",
        "Hiervoor houdt het Q-learning algoritme een matrix bij dat de reward van actie in een state bepaald.\n",
        "In een verkenningsfase laten we toe dat er sub-optimale keuzes genomen worden.\n",
        "Nadat dit lang genoeg gerund heeft, gaan we over naar een exploitation fase waarbij enkel de beste keuzes genomen worden.\n",
        "\n",
        "Om te tonen hoe je het Q-learning algoritme kan implementeren, kan je gebruik maken van het gymnasium package.\n",
        "Dit bevat heel wat eenvoudige environments van spelletjes in python die hiervoor gebruikt kunnen worden.\n",
        "De bron voor onderstaande code komt van een [tutorial van de library](https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install gymnasium\n",
        "%pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.utils.play import play\n",
        "\n",
        "env = gym.make(\"Blackjack-v1\", sab=True, render_mode='rgb_array')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reset the environment to get the first observation\n",
        "done = False\n",
        "observation, info = env.reset()\n",
        "print(observation, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that our observation is a 3-tuple consisting of 3 values:\n",
        "\n",
        "-  The players current sum\n",
        "-  Value of the dealers face-up card\n",
        "-  Boolean whether the player holds a usable ace (An ace is usable if it\n",
        "   counts as 11 without busting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample a random action from all valid actions\n",
        "action = env.action_space.sample()\n",
        "# action=1\n",
        "\n",
        "# execute the action in our environment and receive infos from the environment\n",
        "next_state, reward, terminated, truncated, info = env.step(action)\n",
        "print(observation)\n",
        "print(reward)\n",
        "print(terminated)   # geeft aan of het spel over is\n",
        "print(truncated)    # ook gedaan als dit true is\n",
        "print(info)\n",
        "\n",
        "\n",
        "# observation=(24, 10, False)\n",
        "# reward=-1.0\n",
        "# terminated=True\n",
        "# truncated=False\n",
        "# info={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dit is de AI/bot\n",
        "class BlackjackAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        discount_factor: float = 0.95,\n",
        "    ):\n",
        "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
        "        of state-action values (q_values), a learning rate and an epsilon.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate\n",
        "            initial_epsilon: The initial epsilon value\n",
        "            epsilon_decay: The decay for epsilon\n",
        "            final_epsilon: The final epsilon value\n",
        "            discount_factor: The discount factor for computing the Q-value\n",
        "        \"\"\"\n",
        "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "        self.training_error = []\n",
        "\n",
        "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
        "        \"\"\"\n",
        "        Returns the best action with probability (1 - epsilon)\n",
        "        otherwise a random action with probability epsilon to ensure exploration.\n",
        "        \"\"\"\n",
        "        # with probability epsilon return a random action to explore the environment\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "\n",
        "        # with probability (1 - epsilon) act greedily (exploit)\n",
        "        else:\n",
        "            return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: tuple[int, int, bool],\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple[int, int, bool],\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
        "        temporal_difference = (\n",
        "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
        "        )\n",
        "\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):            # in het begin grote epsilon -> neem veel random zetten, later kleinere epsilon zodat er vooral de beste zet genomen wordt\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_episodes = 100000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
        "final_epsilon = 0.1\n",
        "\n",
        "agent = BlackjackAgent(\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # play one episode\n",
        "    while not done:\n",
        "        action = agent.get_action(obs)\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # update the agent\n",
        "        agent.update(obs, action, reward, terminated, next_obs)     # fit\n",
        "\n",
        "        # update if the environment is done and the current obs\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "\n",
        "    agent.decay_epsilon()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rolling_length = 500\n",
        "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
        "axs[0].set_title(\"Episode rewards\")\n",
        "# compute and assign a rolling average of the data to provide a smoother graph\n",
        "reward_moving_average = (\n",
        "    np.convolve(\n",
        "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
        "    )\n",
        "    / rolling_length\n",
        ")\n",
        "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
        "axs[1].set_title(\"Episode lengths\")\n",
        "length_moving_average = (\n",
        "    np.convolve(\n",
        "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n",
        "    )\n",
        "    / rolling_length\n",
        ")\n",
        "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
        "axs[2].set_title(\"Training Error\")\n",
        "training_error_moving_average = (\n",
        "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n",
        "    / rolling_length\n",
        ")\n",
        "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_grids(agent, usable_ace=False):\n",
        "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
        "    # convert our state-action values to state values\n",
        "    # and build a policy dictionary that maps observations to actions\n",
        "    state_value = defaultdict(float)\n",
        "    policy = defaultdict(int)\n",
        "    for obs, action_values in agent.q_values.items():\n",
        "        state_value[obs] = float(np.max(action_values))\n",
        "        policy[obs] = int(np.argmax(action_values))\n",
        "\n",
        "    player_count, dealer_count = np.meshgrid(\n",
        "        # players count, dealers face-up card\n",
        "        np.arange(12, 22),\n",
        "        np.arange(1, 11),\n",
        "    )\n",
        "\n",
        "    # create the value grid for plotting\n",
        "    value = np.apply_along_axis(\n",
        "        lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    value_grid = player_count, dealer_count, value\n",
        "\n",
        "    # create the policy grid for plotting\n",
        "    policy_grid = np.apply_along_axis(\n",
        "        lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
        "        axis=2,\n",
        "        arr=np.dstack([player_count, dealer_count]),\n",
        "    )\n",
        "    return value_grid, policy_grid\n",
        "\n",
        "\n",
        "def create_plots(value_grid, policy_grid, title: str):\n",
        "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
        "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
        "    player_count, dealer_count, value = value_grid\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    # plot the state values\n",
        "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
        "    ax1.plot_surface(\n",
        "        player_count,\n",
        "        dealer_count,\n",
        "        value,\n",
        "        rstride=1,\n",
        "        cstride=1,\n",
        "        cmap=\"viridis\",\n",
        "        edgecolor=\"none\",\n",
        "    )\n",
        "    plt.xticks(range(12, 22), range(12, 22))\n",
        "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
        "    ax1.set_title(f\"State values: {title}\")\n",
        "    ax1.set_xlabel(\"Player sum\")\n",
        "    ax1.set_ylabel(\"Dealer showing\")\n",
        "    ax1.zaxis.set_rotate_label(False)\n",
        "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
        "    ax1.view_init(20, 220)\n",
        "\n",
        "    # plot the policy\n",
        "    fig.add_subplot(1, 2, 2)\n",
        "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
        "    ax2.set_title(f\"Policy: {title}\")\n",
        "    ax2.set_xlabel(\"Player sum\")\n",
        "    ax2.set_ylabel(\"Dealer showing\")\n",
        "    ax2.set_xticklabels(range(12, 22))\n",
        "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
        "\n",
        "    # add a legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
        "        Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\"),\n",
        "    ]\n",
        "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
        "    return fig\n",
        "\n",
        "\n",
        "# state values & policy with usable ace (ace counts as 11)\n",
        "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
        "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# state values & policy without usable ace (ace counts as 1)\n",
        "value_grid, policy_grid = create_grids(agent, usable_ace=False)\n",
        "fig2 = create_plots(value_grid, policy_grid, title=\"Without usable ace\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gym.utils.play.play(env, keys_to_action={\"w\":0, \"s\":1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL in neural networks\n",
        "\n",
        "Het gebruik van Q-learning werkt goed als het aantal states en acties beperkt zijn.\n",
        "Dit is echter zelden het geval, denk bijvoorbeeld aan een continue variabele zoals snelheid of locatie.\n",
        "\n",
        "Een oplossing hiervoor is om de action-value functie die in Q-learning geoptimaliseerd wordt te benaderen ipv exact te berekenen.\n",
        "Dit kan bijvoorbeeld door middel van een neuraal netwerk te gebruiken.\n",
        "Er zijn verschillende model-structuren die hiervoor ontwikkeld zijn zoals:\n",
        "- DQN (onderwerp van onderstaande demo)\n",
        "- REINFORCE\n",
        "- DDPG\n",
        "- TD3\n",
        "- PPO\n",
        "- SAC\n",
        "\n",
        "Voor we beginnen met het uitwerken van een model.\n",
        "Bekijk [deze tutorial](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial) en beantwoord de volgende vragen:\n",
        "- Wat is de state en wat zijn de mogelijke acties?\n",
        "- Wat is de structuur van het gebruikte DQN?\n",
        "- Zijn er nieuwe hyperparameters gebruikt?\n",
        "- Welke metriek wordt er gebruikt en waar wordt deze berekend?\n",
        "- Hoe worden de gewichten aangepast?\n",
        "- Waarvoor wordt de ReplayBuffer gebruikt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Antwoord:**\n",
        "- Vraag 1:\n",
        "    - State: de positie en snelheid van het karretje en de hoek/hoeksnelheid van de staaf.\n",
        "    - Acties: Beweeg naar links en beweeg naar rechts\n",
        "- Vraag 2: Er zijn drie lagen met respectievelijk 100, 50 en 2 neuronen. Het is belangrijk dat het aantal neuronen in de laatste laag overeenkomt met het aantal acties.\n",
        "- Vraag 3: De enige nieuwe hyperparameter bij het aanmaken van het neuraal netwerk is de initialiser. De hidden lagen gebruiken een VarianceScaler als kernel-initalisator wat inhoudt dat ze gesampled worden uit een Normaalverdeling. De outputlayer gebruikt een RandomUniform kernel-initializer (sample de gewichten uit een uniforme verdeling) en een constante waarde als bias-initializer\n",
        "- Vraag 4: De average return wordt hiervoor gebruikt en deze wordt [hier](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial#metrics_and_evaluation) berekend. De return is de tijd dat de staaf omhoog blijft (1 voor elke tijdstap)\n",
        "- Vraag 5 en 6: Je laadt het netwerk wat lopen, de uitgevoerde acties en bekomen rewards worden opgeslaan in de ReplayBuffer. Batches of data worden uit de replaybuffer gehaald om het netwerk te trainen op basis van de gemiddelde return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Schrijf nu zelf de nodige code om het DQN-model toe te passen op het \"Mountain Car\" environment van gymnasium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.utils.play import play\n",
        "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
        "play(env, keys_to_action={\"a\": 0, \"s\": 1, \"d\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wat is de activatiefunctie van de laatste laag en waarom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import tensorflow as tf\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# deze klasse bevat het neuraal netwerk\n",
        "class DQNetwork:\n",
        "\n",
        "    def __init__(self, state_size, action_size, layer_sizes=(100, 50),\n",
        "                 learning_rate=0.0001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        states = tf.keras.layers.Input(shape=(self.state_size,), name='states')\n",
        "        net = states\n",
        "        # hidden layers\n",
        "\n",
        "        for layer_count in range(len(self.layer_sizes)):\n",
        "            net = tf.keras.layers.Dense(units=self.layer_sizes[layer_count], activation=\"relu\")(net)\n",
        "            \n",
        "        # dit is lineair want we willen de Q-functie gaan benaderen - deze waarden komen rewards/returns en deze kunnen negatief, positief zijn en groter dan 1\n",
        "        # dit lijkt dus op regressie dus nemen we een lineaire activatie functie\n",
        "        actions = tf.keras.layers.Dense(units=self.action_size, activation='linear',\n",
        "                                 name='raw_actions')(net)\n",
        "\n",
        "        self.model = tf.keras.models.Model(inputs=states, outputs=actions)\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
        "        self.model.compile(loss='mse', optimizer=self.optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "# dit is de agent of de bot, deze bepaald hoe er geleerd moet worden\n",
        "class DDQNAgent:\n",
        "\n",
        "    def __init__(self, env, buffer_size=int(1e5), batch_size=64, gamma=0.99, tau=1e-3, lr=5e-4, callbacks=()):\n",
        "        self.env = env\n",
        "        #self.env.seed(1024)      # dit zal niet meer werken met gymnasium versie\n",
        "        # neurale netwerken trainen efficienter als je met grotere batches werkt -> dus update de gewichten maar na dit aantal acties\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.Q_targets = 0.0\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.callbacks = callbacks\n",
        "\n",
        "        layer_sizes = [20, 5]\n",
        "\n",
        "        print(\"Initialising DDQN Agent with params : {}\".format(self.__dict__))\n",
        "\n",
        "        # Make local & target model\n",
        "        # we maken er 2 om tijdens het trainen nog steeds toegang te hebben tot de oude gewichten (target is de backup van local)\n",
        "        print(\"Initialising Local DQNetwork\")\n",
        "        self.local_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                       layer_sizes=layer_sizes,\n",
        "                                       learning_rate=lr)\n",
        "\n",
        "        print(\"Initialising Target DQNetwork\")\n",
        "        self.target_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                        layer_sizes=layer_sizes,\n",
        "                                        learning_rate=lr)\n",
        "\n",
        "        # dit is een trucje om neurale netwerken beter te laten trainen in Reinforcement learning\n",
        "        self.memory = ReplayBuffer(buffer_size=buffer_size, batch_size=batch_size)\n",
        "\n",
        "    def reset_episode(self):\n",
        "        state = self.env.reset()\n",
        "        self.last_state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action, reward, next_state, done):\n",
        "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
        "\n",
        "        # leer in batches\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            # update de gewichten van het neuraal netwerk\n",
        "            self.learn(experiences, self.gamma)\n",
        "\n",
        "        self.last_state = next_state\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        state = np.reshape(state, [-1, self.state_size])\n",
        "        action = self.local_network.model.predict(state)\n",
        "\n",
        "        if random.random() > eps:\n",
        "            # dit is niet random -> greedy approach\n",
        "            # de beste actie volgens het neuraal netwerk\n",
        "            return np.argmax(action)\n",
        "        else:\n",
        "            # verkennend, exploring, random acties\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # voor elke actie in de batch\n",
        "        for itr in range(len(states)):\n",
        "            # inputs goed zetten\n",
        "            state, action, reward, next_state, done = states[itr], actions[itr], rewards[itr], next_states[itr], dones[\n",
        "                itr]\n",
        "            state = np.reshape(state, [-1, self.state_size])\n",
        "            next_state = np.reshape(next_state, [-1, self.state_size])\n",
        "\n",
        "            # dit berekend de nieuwe Q's\n",
        "            self.Q_targets = self.local_network.model.predict(state, verbose=0)\n",
        "            if done:\n",
        "                self.Q_targets[0][action] = reward\n",
        "            else:\n",
        "                next_Q_target = self.target_network.model.predict(next_state, verbose=0)[0]\n",
        "                # next_Q_target zijn de toekomstige Q-waarden\n",
        "                self.Q_targets[0][action] = (reward + gamma * np.max(next_Q_target))\n",
        "\n",
        "            self.local_network.model.fit(state, self.Q_targets, epochs=1, verbose=0, callbacks=self.callbacks)\n",
        "\n",
        "    # dit is om het target te updaten met de nieuwe gewichten van het local\n",
        "    def update_target_model(self):\n",
        "        self.target_network.model.set_weights(self.local_network.model.get_weights())\n",
        "\n",
        "# double ended queue (deque)\n",
        "# (ChatGPT) Here's why a replay buffer is beneficial in DQN:\n",
        "# Stability: Training on a randomly sampled batch of experiences helps to break the temporal correlation between consecutive experiences. This reduces the risk of the learning process being influenced by the order in which experiences are encountered.\n",
        "# Data Efficiency: Reusing past experiences allows the agent to learn more from its limited set of interactions with the environment. This is particularly useful when data is expensive or time-consuming to collect.\n",
        "# Sample Efficiency: The replay buffer helps the agent learn from a more diverse set of experiences, potentially preventing the network from getting stuck in local minima.\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    # sla de gegevens op\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    # geef me een lijst van tijdstippen\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = np.vstack([e.state for e in experiences if e is not None])\n",
        "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
        "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
        "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
        "        dones = np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "def train_model(n_episodes=200, eps_start=1.0, eps_end=0.001, eps_decay=0.9, target_reward=1000):\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "    print(\"Starting model training for {} episodes.\".format(n_episodes))\n",
        "    consolidation_counter = 0\n",
        "    # for lusje om het aantal spelletjes te simuleren\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        init_time = time()\n",
        "        state = agent.reset_episode()\n",
        "        score = 0\n",
        "        done = False\n",
        "        # simuleer het spelletje hier\n",
        "        while not done:\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                # als het spel gedaan is, gebruik de nieuwe gewichten voor het volgend spel\n",
        "                agent.update_target_model()\n",
        "                break\n",
        "        time_taken = time() - init_time\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        # debugging output / progress reports / checkpoints bijhouden\n",
        "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}'\n",
        "                     '\\tEffective Epsilon: {:.3f}\\tTime Taken: {:.2f} sec'.format(\n",
        "            i_episode, np.mean(scores_window), score, state[0], np.mean(agent.Q_targets), eps, time_taken))\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\n",
        "                'Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}\\tTime Taken: {:.2f} sec '.format(\n",
        "                    i_episode, np.mean(scores_window), score, state[0], np.mean(agent.Q_targets), time_taken))\n",
        "            agent.local_network.model.save('save/{}_local_model_{}.h5'.format(env_name, initial_timestamp))\n",
        "            agent.target_network.model.save('save/{}_target_model_{}.h5'.format(env_name, initial_timestamp))\n",
        "        if np.mean(scores_window) >= target_reward:\n",
        "            consolidation_counter += 1\n",
        "            if consolidation_counter >= 5:\n",
        "                print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                                    \" Training ran for total of {} epsiodes\".format(\n",
        "                    np.mean(scores_window), 100, i_episode))\n",
        "                return scores\n",
        "        else:\n",
        "            consolidation_counter = 0\n",
        "    print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                        \" Training ran for total of {} epsiodes\".format(\n",
        "        np.mean(scores_window), 100, n_episodes))\n",
        "    return scores\n",
        "\n",
        "\n",
        "def play_model(actor, env_render=False, return_render_img=False):\n",
        "    state = env.reset()\n",
        "    print(\"Start state : {}\".format(state))\n",
        "    score = 0\n",
        "    done = False\n",
        "    images = []\n",
        "    R = 0\n",
        "    t = 0\n",
        "    while not done:\n",
        "        if env_render:\n",
        "            if return_render_img:\n",
        "                images.append(env.render(\"rgb_array\"))\n",
        "            else:\n",
        "                env.render()\n",
        "        state = np.reshape(state, [-1, env.observation_space.shape[0]])\n",
        "        action = actor.predict(state, verbose=0)\n",
        "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "        R += reward\n",
        "        t += 1\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            return score, images\n",
        "    return 0, images\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialising DDQN Agent with params : {'env': <TimeLimit<OrderEnforcing<PassiveEnvChecker<MountainCarEnv<MountainCar-v0>>>>>, 'batch_size': 64, 'gamma': 0.99, 'tau': 0.001, 'Q_targets': 0.0, 'state_size': 2, 'action_size': 3, 'callbacks': []}\n",
            "Initialising Local DQNetwork\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialising Target DQNetwork\n",
            "Starting model training for 1 episodes.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jens.baetens3\\ODISEE\\DIGITAL - OPLINF\\AJ23-24\\OPO's\\Bachelors\\Machine Learning\\Leerstof\\Week 8\\Week 10.ipynb Cell 24\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m agent \u001b[39m=\u001b[39m DDQNAgent(env, buffer_size\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, callbacks\u001b[39m=\u001b[39m[])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m scores \u001b[39m=\u001b[39m train_model(n_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, target_reward\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m110\u001b[39;49m, eps_decay\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m111\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\jens.baetens3\\ODISEE\\DIGITAL - OPLINF\\AJ23-24\\OPO's\\Bachelors\\Machine Learning\\Leerstof\\Week 8\\Week 10.ipynb Cell 24\u001b[0m line \u001b[0;36mtrain_model\u001b[1;34m(n_episodes, eps_start, eps_end, eps_decay, target_reward)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# simuleer het spelletje hier\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mact(state, eps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     agent\u001b[39m.\u001b[39mstep(action, reward, next_state, done)\n",
            "\u001b[1;32mc:\\Users\\jens.baetens3\\ODISEE\\DIGITAL - OPLINF\\AJ23-24\\OPO's\\Bachelors\\Machine Learning\\Leerstof\\Week 8\\Week 10.ipynb Cell 24\u001b[0m line \u001b[0;36mDDQNAgent.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, state, eps\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(state, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate_size])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_network\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m>\u001b[39m eps:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39m# dit is niet random -> greedy approach\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jens.baetens3/ODISEE/DIGITAL%20-%20OPLINF/AJ23-24/OPO%27s/Bachelors/Machine%20Learning/Leerstof/Week%208/Week%2010.ipynb#X33sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39m# de beste actie volgens het neuraal netwerk\u001b[39;00m\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
            "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "#train\n",
        "env_name = \"MountainCar-v0\"\n",
        "env = gym.make(env_name)\n",
        "agent = DDQNAgent(env, buffer_size=100000, gamma=0.99, batch_size=64, lr=0.0001, callbacks=[])\n",
        "scores = train_model(n_episodes=1, target_reward=-110, eps_decay=0.9)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = \"MountainCar-v0_local_model.h5\"\n",
        "total_iterations = 100\n",
        "expected_reward = -110\n",
        "\n",
        "#Test\n",
        "test_scores = []\n",
        "print(\"Loading the saved model from '{}'\".format(model))\n",
        "actor = tf.keras.models.load_model('{}'.format(model))\n",
        "print(\"Now running model test for {} iterations with expected reward >= {}\".format(\n",
        "    total_iterations, expected_reward))\n",
        "frames = play_model(actor, True, True)[1]\n",
        "for itr in range(1, total_iterations + 1):\n",
        "    score = play_model(actor, True)[0]\n",
        "    test_scores.append(score)\n",
        "    print(\"Iteration: {} Score: {}\".format(itr, score))\n",
        "avg_reward = np.mean(test_scores)\n",
        "print(\"Total Avg. Score over {} consecutive iterations : {}\".format(total_iterations,\n",
        "                                                                                 avg_reward))\n",
        "if avg_reward >= expected_reward:\n",
        "    print(\"Env. solved successfully.\")\n",
        "else:\n",
        "    print(\"Agent failed to solve the env.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
